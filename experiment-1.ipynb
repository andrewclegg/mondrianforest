{
 "metadata": {
  "name": "",
  "signature": "sha256:bf8b60705a12e30c87e565915ce49c1c2f262741ba74d56ef71040a6865bcd09"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Mondrian Forests\n",
      "\n",
      "This is a reimplementation of Mondrian Forests as described here:\n",
      "\n",
      "**Mondrian Forests: Efficient Online Random Forests**\n",
      "\n",
      "Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh\n",
      "\n",
      "http://arxiv.org/abs/1406.2673\n",
      "\n",
      "It partly uses code from the original authors' implementation at\n",
      "\n",
      "https://github.com/balajiln/mondrianforest\n",
      "\n",
      "but changes a few things along the way too. Key differences:\n",
      "\n",
      "* Assume FAST_MODE (no log-likelihood) for now\n",
      "* Classification only (they never finished regression)\n",
      "* Always assume we're training incrementally\n",
      "* `update_posterior_node_incremental` just becomes `update_label_counts`\n",
      "* `add_training_points_to_node` just becomes `MondrianNode.update`\n",
      "* Pushed as much metadata as possible into the node object, to avoid having lots of separate dicts etc. maintained by the tree (hence renaming it `MondrianNode`)\n",
      "* `data` (an array) refers to one batch, instead of having a single design matrix in memory and indexing it with `train_ids`\n",
      "* `labels` is a vector of class labels for the rows in `data`\n",
      "* We don't assume you'll provide the first batch of training data when initializing a tree, but we do need to know the number of dimensions and number of labels\n",
      "* It works in parallel :-)\n",
      "\n",
      "Perhaps most different of all:\n",
      "\n",
      "We don't keep all the input data, as this won't scale. But this means we can't move existing data down into the new leaf nodes when we split a leaf node.\n",
      "\n",
      "This is the plan: When the tree \"grows\" through randomly splitting a leaf node, keep its existing class frequencies where they are, and only use the two new children for *newly-arriving* data. Then take these into account when calculating the predictive posteriors (appendix A of paper).\n",
      "\n",
      "Here are my original thoughts on this, that I almost sent to the authors, but decided to try first.\n",
      "\n",
      "```\n",
      "One thing I missed on reading the paper, is that when you bisect a leaf node through a \"grow\" (sampling) operation,\n",
      "you need to assign all its data points to one of the new children.\n",
      "\n",
      "This means you can't just keep summary statistics for the data in a block, i.e. class frequencies. You need to hold\n",
      "on to the raw training data in order to divide it, yes? Which is a problem for online learning over streaming data\n",
      "or just \"big data\" in general.\n",
      "\n",
      "So... Perhaps you could entirely forget about keeping the existing data and propagating it down to the new child\n",
      "nodes. If you just kept class frequencies at each node, maybe you could use these in the \"update posterior counts\"\n",
      "step for internal nodes, alongside the child node counts.\n",
      "\n",
      "(Any data that came in *after* the split would go all the way down to the leaf nodes though.)\n",
      "```\n",
      "\n",
      "For the time being, though, it's using a much simpler scoring system. Walk down the tree until you find the box that the test example would have been in, and go for the majority class. If you don't find a box, or it's empty, walk back up until you find the nearest ancestor with some instances in.\n",
      "\n",
      "Averaged across a few trees, even as few as ~10, this actually does pretty well.\n",
      "\n",
      "## Running this demo\n",
      "\n",
      "If a `default` IPython cluster is available, it runs in parallel automatically, with as many trees in the forest as you have engines in the cluster. Otherwise, it runs in serial on a single core, with 3 trees. See the notes in the third cell.\n",
      "\n",
      "For best results with small forests, use an odd number of trees.\n",
      "\n",
      "## TODO\n",
      "\n",
      "### Must-haves\n",
      "\n",
      "* Proper probabilistic scoring\n",
      "* Types for all numpy arrays\n",
      "* Error checking\n",
      "* Work out exactly what effect `budget` is having\n",
      "* Introspection and prettyprinting tools\n",
      "* Work out which members are \"private\" and rename them with underscores\n",
      "* Proper tests...\n",
      "\n",
      "### Useful features\n",
      "\n",
      "* Allow multiple trees per engine (i.e. each engine runs a forest, not a tree)\n",
      "* Instance subsampling\n",
      "* Feature subsampling/bootstrapping/bagging\n",
      "* Feature hashing/random projection/LSH\n",
      "* Train-on-fail mode\n",
      "* Loss functions, regret, class weighting?\n",
      "\n",
      "### Blue sky\n",
      "\n",
      "* Regression\n",
      "* Clustering? (I mean, it basically *is* clustering, already...)\n",
      "* Semi-supervised would be really easy too\n",
      "* Adaptivity -- non-stationary distributions, recency effects etc.\n",
      "* Auto-pruning/rebalancing -- stop trees either stagnating or growing out of control in the long term\n",
      "* What about when the true number of classes isn't known in advance?\n",
      "\n",
      "The last three are really closely related.\n",
      "\n",
      "We could probably tackle a lot of these problems by \"chopping down\" older trees -- either randomly or according to a heuristic (e.g. least-recently-modified) -- so new ones can grow in their place. Maybe varying the budget and discount values would be useful too.\n",
      "\n",
      "Perhaps you could calculate the posterior class probabilities over the entire 'dead' tree, and use these as the priors for the new tree that will grow in its place.\n",
      "\n",
      "You could also use this kind of approach to tune P/R tradeoff, chase a particular objective function, do reinforcement learning... If you squint, they start to look a little like GAs.\n",
      "\n",
      "* Ontology learning\n",
      "* Grammar induction\n",
      "* Topic modelling\n",
      "\n",
      "Use cases that are fundamentally tree-shaped! Hmmm."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run 'mondrian.py'\n",
      "\n",
      "import random\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "from IPython.parallel import Client\n",
      "try:\n",
      "    rc = Client()\n",
      "    dview = rc[:]\n",
      "    print('Using default IPython cluster with %d engines' % len(dview))\n",
      "except FileNotFoundError:\n",
      "    rc = None\n",
      "    dview = None\n",
      "    print('Using single-core implementation')\n",
      "\n",
      "DEFAULT_BUDGET = 1000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using single-core implementation\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import gzip\n",
      "import os\n",
      "import pandas as pd\n",
      "from sklearn import datasets\n",
      "from sklearn import cross_validation as cv\n",
      "\n",
      "# For speed later...\n",
      "test_file = '/home/vagrant/data/covtype.pkl.gz'\n",
      "if os.path.exists(test_file):\n",
      "    with gzip.GzipFile(test_file, 'rb') as z:\n",
      "        dataset = pickle.load(z)\n",
      "else:\n",
      "    dataset = datasets.fetch_covtype() # Can be quite slow\n",
      "    with gzip.GzipFile(test_file, 'wb') as z:\n",
      "        pickle.dump(dataset, z)\n",
      "        \n",
      "dataset.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(581012, 54)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%time\n",
      "#%%prun\n",
      "\n",
      "n_iter = 10\n",
      "test_size = 10\n",
      "folds = cv.StratifiedShuffleSplit(dataset.target, n_iter=n_iter, test_size=test_size, train_size=1000)\n",
      "hits = []\n",
      "\n",
      "data = dataset.data\n",
      "target = dataset.target - 1 # they are 1-indexed\n",
      "\n",
      "if dview is None:\n",
      "    # Run with a single-threaded forest with 3 trees\n",
      "    # FIXME put this back to 3\n",
      "    mf = MondrianForest(1, data.shape[1], len(np.unique(target)), budget=DEFAULT_BUDGET, scoring='simple')\n",
      "else:\n",
      "    # Run with a parallel forest on IPython cluster\n",
      "    mf = ParallelMondrianForest(dview, data.shape[1], len(np.unique(target)), budget=DEFAULT_BUDGET, scoring='simple')\n",
      "\n",
      "def run_test():\n",
      "    for i, (train, test) in enumerate(folds):\n",
      "        train_X = data[train]\n",
      "        train_y = target[train]\n",
      "        test_X = data[test]#[0:1,:]\n",
      "        test_y = target[test]#[0:1]\n",
      "        mf.update(train_X, train_y)\n",
      "        preds = mf.predict(test_X)\n",
      "        yhat = preds.argmax(axis=1)\n",
      "        assert test_y.shape == yhat.shape\n",
      "        for j in range(len(preds)):\n",
      "            idx = (i * test_size) + j\n",
      "            print(idx, '- Predicted:', yhat[j], 'Actual:', test_y[j],\n",
      "                  '[', ' '.join(['%.3f' % x for x in preds[j]]), ']')\n",
      "            hits.append(yhat[j] == test_y[j])\n",
      "\n",
      "run_test()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = mf.trees[0].root.left.left.left.left.left.left.left\n",
      "%timeit get_posterior_class_probs(n, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.random.random((10, 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit normalize(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000 loops, best of 3: 10.6 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit a / a.sum(axis=1)[:,np.newaxis]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 19.1 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = np.random.random(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series(hits).cumsum().plot() # Predictions over time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(hits)/len(hits) # Overall accuracy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}